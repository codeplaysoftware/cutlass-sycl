# Benchmarks for required shapes for first Pytorch release

# TODO(codeplay): Confirm that these are the right layout for what's required
# q_mm 1,8 4096 4096
PvcGemmBF16BF16FP32_RCR_6 --bm_name=q_mm --m=1 --k=4096 --n=4096
PvcGemmBF16BF16FP32_RCR_6 --bm_name=q_mm --m=8 --k=4096 --n=4096

# k_mm 1,8 4096 1024
# v_mm 1,8 4096 1024
PvcGemmBF16BF16FP32_RCR_6 --bm_name=k_v_mm --m=1 --k=4096 --n=1024
PvcGemmBF16BF16FP32_RCR_6 --bm_name=k_v_mm --m=8 --k=4096 --n=1024

# qkv_fusion 1,8 4096 6144 = (4096 + 1024 + 1024)
PvcGemmBF16BF16FP32_RCR_6 --bm_name=qkv_fusion --m=1 --k=4096 --n=6144
PvcGemmBF16BF16FP32_RCR_6 --bm_name=qkv_fusion --m=8 --k=4096 --n=6144

# mm_common 1,8 4096 4096
PvcGemmBF16BF16FP32_RCR_6 --bm_name=mm_common --m=1 --k=4096 --n=4096
PvcGemmBF16BF16FP32_RCR_6 --bm_name=mm_common --m=8 --k=4096 --n=4096

# mm_silu 1,8 4096 14336
PvcGemmBF16BF16FP32_RCR_6_silu --bm_name=mm_silu --m=1 --k=4096 --n=14336
PvcGemmBF16BF16FP32_RCR_6_silu --bm_name=mm_silu --m=8 --k=4096 --n=14336

# mm_mul 1,8 4096 14336
PvcGemmBF16BF16FP32_RCR_6_mul --bm_name=mm_mul --m=1 --k=4096 --n=14336
PvcGemmBF16BF16FP32_RCR_6_mul --bm_name=mm_mul --m=8 --k=4096 --n=14336

# mm_fusion 1,8 4096 28672 = (2x14336)
PvcGemmBF16BF16FP32_RCR_6 --bm_name=mm_fusion --m=1 --k=4096 --n=28672
PvcGemmBF16BF16FP32_RCR_6 --bm_name=mm_fusion --m=8 --k=4096 --n=28672

# mm_add 1,8 14336 4096
PvcGemmBF16BF16FP32_RCR_6 --bm_name=mm_add --m=1 --k=14336 --n=4096 --beta=1
PvcGemmBF16BF16FP32_RCR_6 --bm_name=mm_add --m=8 --k=14336 --n=4096 --beta=1

# lmhead_mm 1,8 4096 128256
PvcGemmBF16BF16FP32_RCR_6 --bm_name=lmhead_mm --m=1 --k=4096 --n=128256
PvcGemmBF16BF16FP32_RCR_6 --bm_name=lmhead_mm --m=8 --k=4096 --n=128256

# Flash attention extend
# cached(0) input(1024) query(1, 1024, 128, 192) key(1, 2048, 128, 192) value(1, 2048, 128, 128)
PvcFMHABF16BF16FP32_RCR_h128_Causal_FixedLen --bm_name=attention_extend --seq_len_qo=1024 --seq_len_kv=2048 --batch=1 --num_heads_q=128 --num_heads_kv=128 --head_size_qk=192 --head_size_vo=128
PvcFMHABF16BF16FP32_RCR_h128_NonCausal_FixedLen --bm_name=attention_extend --seq_len_qo=1024 --seq_len_kv=2048 --batch=1 --num_heads_q=128 --num_heads_kv=128 --head_size_qk=192 --head_size_vo=128

# cached(0) input(1024) query(1, 1024, 32, 128) key(1, 2048, 8, 128) value(1, 2048, 8, 128)
PvcFMHABF16BF16FP32_RCR_h64_Causal_FixedLen --bm_name=attention_extend --seq_len_qo=1024 --seq_len_kv=2048 --batch=1 --num_heads_q=32 --num_heads_kv=8 --head_size_qk=128 --head_size_vo=128
PvcFMHABF16BF16FP32_RCR_h64_NonCausal_FixedLen --bm_name=attention_extend --seq_len_qo=1024 --seq_len_kv=2048 --batch=1 --num_heads_q=32 --num_heads_kv=8 --head_size_qk=128 --head_size_vo=128
